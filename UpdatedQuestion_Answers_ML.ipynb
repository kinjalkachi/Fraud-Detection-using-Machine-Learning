{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Fraud from Euron Email\n",
    "\n",
    "1)Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]\n",
    "\n",
    "In this project my work is to find employees involved in Enron Fraud case. Machine Learning made it easy , I could make new features, guess outliers, engineer the dataset using classifiers to do the same.\n",
    "\n",
    "There are 146 people in the data set , With 21 Features, 11 financial features and 6 email features.\n",
    "There are 6 features with more than 50% missing values, they are:\n",
    "'deferral_payments', 'loan_advances', 'restricted_stock_deferred', 'deferred_income', 'long_term_incentive', 'director_fees'\n",
    "\n",
    "##### Updated information: There are 18 pois, and 128 non poi\n",
    "\n",
    "TOTAL column as the major outlier in this dataset,\n",
    "Looking the the XLS we found it is the EXCEL artifact and should be removed. Another outlier is also determined, \n",
    "THE TRAVEL AGENCY IN THE PARK this record did not represent an individual. Both of these should be removed.\n",
    "I have choosen exercised stock options and bonus as these are two features that can be exercised by pois.\n",
    "While working on data I found that people enjoying high bonus and exercised stock options are outliers , but it is necessary to keep them as they provide an important piece of information.\n",
    "\n",
    "2)What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “intelligently select features”, “properly scale features”]\n",
    "\n",
    "\n",
    "I have choosen exercised stock options and bonus as these are two features that can be exercised by pois.\n",
    "I have also choosen 'from_poi_to_this_person' and 'from_this_person_to_poi' are very important features too, beacuse they show the maximum exchange of emails between two parties , that could be possibly between the pois.\n",
    "\n",
    "I have created my own two features 'extra_pay_poi' : total_payment -salary ,\n",
    "because might happen that pois have much more total payment than there base salary , they might be enjoying otherhigh sources of income through options like deffered income , high bonus.\n",
    "\n",
    "Updated information : As suggested by previous reviewer I used RFECV module , to get an optimal number of features to be used.\n",
    "Hence optimal number of features is 15 . I have used SelectKBest to obtain the best feature scores of the selected 15 features.\n",
    "At this point I observed that 15 best features included my newly created features.Scores of all the features are as follows\n",
    "\n",
    "              Feature                            Score\n",
    "\n",
    "        'exercised_stock_options'               24.815079733218194\n",
    "        'total_stock_value'                     24.182898678566879\n",
    "        'bonus'                                 20.792252047181535\n",
    "        'salary'                                18.289684043404513\n",
    "        'deferred_income',                      11.458476579280369\n",
    "        'long_term_incentive'                   9.9221860131898225\n",
    "        'restricted_stock'                      9.2128106219771002     SELECTED 15 FEATURES\n",
    "        'extra_pay_poi'                         9.0955158262415505\n",
    "        'total_payments'                        8.7727777300916792\n",
    "        'shared_receipt_with_poi'               8.589420731682381\n",
    "        'loan_advances'                         7.1840556582887247\n",
    "        'expenses'                              6.0941733106389453\n",
    "        'from_poi_to_this_person'               5.2434497133749582\n",
    "        'other'                                 4.1874775069953749\n",
    "        'from_this_person_to_poi'               2.3826121082276739\n",
    "********************************************************************************************************************************\n",
    "        'director_fees'                         2.1263278020077054\n",
    "        'to_messages'                           1.6463411294420076\n",
    "        'deferral_payments'                     0.22461127473600989   NOT SELECTED FEATURES\n",
    "        'from_messages'                         0.16970094762175533\n",
    "        'restricted_stock_deferred'             0.065499652909942141\n",
    "        \n",
    "The impact on the algorithm, in terms of performance metrics :\n",
    " Including newly created features extra_pay_poi , F1 score of 15 best features model was 0.1333 , when extra_pay_poi was removed the f1 score dropped down to 0.0\n",
    "\n",
    "3)What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms? \n",
    "\n",
    "I have choosen Gaussian NB , KNearest Classifier and Decision trees, due to following reasons :\n",
    "\n",
    "Classification. When the data are being used to predict a category, supervised learning is also called classification. This is the case when assigning an image as a picture of either a 'cat' or a 'dog'.\n",
    "Decision Tree : A decision tree subdivides a feature space into regions of roughly uniform values Because a feature space can be subdivided into arbitrarily small regions, it's easy to imagine dividing it finely enough to have one data point per regi\n",
    "Gaussian NB : Bayesian methods have a highly desirable quality: they avoid overfitting. They do this by making some assumptions beforehand about the likely distribution of the answer\n",
    "K Nearest Algorithm : The principle behind nearest neighbor methods is to find a predefined number of training samples closest in distance to the new point, and predict the label from these. The number of samples can be a user-defined constant (k-nearest neighbor learning), or vary based on the local density of points (radius-based neighbor learning).\n",
    "\n",
    "K nearest gave highest accuracy of 0.87 , but high accuracy is not always good measure for analysing the quality of data set.\n",
    "But K nearest also has high precision than recall , which shows we can trust the classification judgements made by it. \n",
    "In this example, the algorithm ran has maximum precision, since the one piece of content that it did label was in fact spammy.\n",
    "Decision Tree and Gaussian NB need parameter tunning fortheir less recall and precision.\n",
    "\n",
    "Looking at the importances we notice, we have mostly got low importances which is good because,although\n",
    "a feature might seem unnecessary or less important because of its low (or negative) importance score, it\n",
    "could be the case that it is correlated to other features that can still produce a ‘good’ performance result.\n",
    "\n",
    "In general, features with higher importance scores are more sensitive to random shuffling \n",
    "of their values, which means they are more ‘important’ for prediction,most importanct feature with threshold <0.2 is\n",
    " restricted_stock  0.355319365159\n",
    "\n",
    "About Feature Scaling :\n",
    "So in general - scaling ensures that just because some features are big it won't lead to using them as a main predictor. But in our case all the data points are neccessary and they are good to classify.\n",
    "\n",
    "4)What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? What parameters did you tune? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  [relevant rubric items: “discuss parameter tuning”, “tune the algorithm”]\n",
    "\n",
    "\n",
    "Tuning the parameter of an algorithm means that to customize the algorithm resulting best accuracy, If you dont tune the parameters and use default parameters you may end up getting the accuracy significantly not better than the best accuracy it can produce .\n",
    "The grid search provided by GridSearchCV exhaustively generates candidates from a grid of parameter values specified with the param_grid parameter.I've used gridsearch to tune the parameters of PCA since i've combined it with gaussianNB which dosen't have parameters ,I endeded up tuning only 'n_components' of PCA which resulted in achieving similar accuracy, precision and better Recall than GaussianNb's(no combination) recall.\n",
    "PCA-based anomaly detection - the vast majority of the data falls into a stereotypical distribution; points deviating dramatically from that distribution are suspect\n",
    "I've also tuned PCA with decissiontree classifier to check how the results are diffrent from Decission tree classsifier. I've tuned the parameters 'n_components' for PCA and 'min_samples_split' for decission stee .This resulted in archieving more accuracy but less recall than the decession tree classifier(default).\n",
    "\n",
    "Summary of Classifiers :\n",
    "Classifier                     Accuracy     Precision     Recall     F1 score\n",
    "\n",
    "KNearest Classifier           0.87913       0.69358       0.16750     0.26983\n",
    "Decision Tree                 0.79460       0.22353       0.21850     0.22099\n",
    "Gaussian NB                   0.77047       0.25129       0.36450     0.29749\n",
    "PCA with KNearest Classifier  0.87633       0.66744       0.14450     0.23757\n",
    "PCA with Decision Tree        0.82980       0.36359       0.36850     0.36603\n",
    "PCA with Gaussian NB          0.85300       0.42857       0.30750     0.35808\n",
    "\n",
    "    \n",
    "From the summary it is understood that the KNearest algorithm does not get tunned well , its recall still reamins less than 0.3.\n",
    " \n",
    "        \n",
    "5)What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric items: “discuss validation”, “validation strategy”]\n",
    "\n",
    "Validation : Splitting the data into training and testing datasets.\n",
    "What could go wrong : While spliting the data in test and training sets, we might want to maximize both the sets,the traning data set to maximize best training result and test data set to maximize the validation hence  while adjusting the trade off we may loose much of the data in test sets which could be used for training.\n",
    "Hence we use cross validations , which gives option of k folds, like : suppose out data set has 100 points , and we have\n",
    "k folds =10, hence we have 10 groups with 20 points. the validation will run K times on the data set selecting one of 10 groups as 1 test set and remaining 9 training sets, then the algorithm repeat with another group as testing set and remaining sets as training set , then testing will be on all the 10 sets and its average will be considered as test output .(reference for this context : udacity lesson 14 , quiz 8)\n",
    "In the tester.py file we have performed the same process using StratifiedShuffleSplit Method . Stratified rearrange split cross-approval is a converge of StratifiedKFold and ShuffleSplit, which returns stratified randomized folds. Stratification is the process of rearranging the data as to ensure each fold is a good representative of the whole. For example in a binary classification problem where each class comprises 50% of the data, it is best to arrange the data such that in every fold, each class comprises around half the instances.\n",
    "\n",
    "6)Give at least 2 evaluation metrics and your average performance for each of them. Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]\n",
    "\n",
    "I have used accuracy , precision , recall in the exploration.\n",
    "\n",
    "precision = true positives / (true posotives +false positives)\n",
    "recall = true positives / (true posotives +false negatives)\n",
    "In  K nearest also has high precision than recall , which shows we can trust the classification judgements made by it.\n",
    "The cost of this is that I sometimes get some false positives\n",
    "\n",
    "Accuracy = no. of data points in a class labeled correctly/ all data points in a class.\n",
    "After tunning Gaussian NB the accuracy is as high as >0.9 , this much of high accuracy is never a good sign beacuse of\n",
    "the \"accuracy paradox\". When TP < FP, then accuracy will always increase when we change a classification rule to always output “negative” category. Conversely, when TN < FN, the same will happen when we change our rule to always output “positive”.\n",
    "So what can we do, so we are not tricked into thinking one classifier model is better than other one\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
